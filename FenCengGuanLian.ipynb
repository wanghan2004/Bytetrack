{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847e0265-e34b-4ba6-ba1d-bacddf8fac55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [DEBUG] 步骤 0: 检查关键库版本...\n",
      ">>> [INFO] mmcv version: 1.7.2\n",
      ">>> [INFO] timm version: 0.6.12\n",
      ">>> [INFO] ultralytics version: 8.3.213\n",
      ">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\n",
      ">>> [DEBUG] 步骤 0: 检查完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 1: 开始导入核心库...\n",
      ">>> [DEBUG] 核心库导入成功。\n",
      ">>> [DEBUG] Metric3D 模块导入成功。\n",
      ">>> [DEBUG] 步骤 1: 所有库导入完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 2: 配置模型和文件路径...\n",
      ">>> [DEBUG] 所有文件路径检查通过。\n",
      ">>> [DEBUG] 将要使用的设备: cuda\n",
      ">>> [DEBUG] 步骤 2: 配置完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\n",
      ">>> [INFO] 目标类别 'Car' 已找到, ID为: 0\n",
      ">>> [SUCCESS] Metric3Dv2 模型加载成功！\n",
      ">>> [DEBUG] 步骤 3: 所有模型加载完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 4: 定义视频处理函数...\n",
      ">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 5: 开始执行主程序...\n",
      "\n",
      "--- 开始视频处理 (最终鲁棒深度融合追踪) ---\n",
      ">>> [INFO] 输入视频信息: 1242x374 @ 1.00 FPS, 共 233 帧。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "视频处理进度:   4%|▍         | 9/233 [00:02<01:12,  3.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 275\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>>> [DEBUG] 步骤 5: 开始执行主程序...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 275\u001B[0m     \u001B[43mprocess_video_with_robust_depth_fusion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINPUT_VIDEO_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOUTPUT_VIDEO_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!!! [FATAL ERROR] 在视频处理过程中发生严重错误: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 156\u001B[0m, in \u001B[0;36mprocess_video_with_robust_depth_fusion\u001B[0;34m(input_path, output_path)\u001B[0m\n\u001B[1;32m    154\u001B[0m rgb_frame_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(rgb_frame, metric3d_input_size)\n\u001B[1;32m    155\u001B[0m rgb_torch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(rgb_frame_resized)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n\u001B[0;32m--> 156\u001B[0m pred_output \u001B[38;5;241m=\u001B[39m \u001B[43mmetric3d_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrgb_torch\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m pred_depth_np \u001B[38;5;241m=\u001B[39m pred_output[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    158\u001B[0m pred_depth_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(pred_depth_np, (width, height))\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/__base_model__.py:13\u001B[0m, in \u001B[0;36mBaseDepthModel.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m---> 13\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdepth_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m], output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfidence\u001B[39m\u001B[38;5;124m'\u001B[39m], output\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/dense_pipeline.py:15\u001B[0m, in \u001B[0;36mDensePredModel.forward\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# [f_32, f_16, f_8, f_4]\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:984\u001B[0m, in \u001B[0;36mRAFTDepthNormalDPT5.forward\u001B[0;34m(self, vit_features, **kwargs)\u001B[0m\n\u001B[1;32m    982\u001B[0m     flow_predictions\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclamp(flow_up[:,:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregress_scale \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_val))\n\u001B[1;32m    983\u001B[0m     conf_predictions\u001B[38;5;241m.\u001B[39mappend(flow_up[:,\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m--> 984\u001B[0m     normal_outs\u001B[38;5;241m.\u001B[39mappend(\u001B[43mnorm_normalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflow_up\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    986\u001B[0m outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    987\u001B[0m     prediction\u001B[38;5;241m=\u001B[39mflow_predictions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    988\u001B[0m     predictions_list\u001B[38;5;241m=\u001B[39mflow_predictions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    996\u001B[0m     low_resolution_init\u001B[38;5;241m=\u001B[39mlow_resolution_init,\n\u001B[1;32m    997\u001B[0m )\n\u001B[1;32m    999\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:241\u001B[0m, in \u001B[0;36mnorm_normalize\u001B[0;34m(norm_out)\u001B[0m\n\u001B[1;32m    239\u001B[0m norm_x, norm_y, norm_z, kappa \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(norm_out, \u001B[38;5;241m1\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    240\u001B[0m norm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(norm_x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_y \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_z \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-10\u001B[39m\n\u001B[0;32m--> 241\u001B[0m kappa \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43melu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkappa\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmin_kappa\u001B[49m\n\u001B[1;32m    242\u001B[0m final_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([norm_x \u001B[38;5;241m/\u001B[39m norm, norm_y \u001B[38;5;241m/\u001B[39m norm, norm_z \u001B[38;5;241m/\u001B[39m norm, kappa], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m final_out\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. 关键依赖库检查 (用于调试)\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 0: 检查关键库版本...\")\n",
    "try:\n",
    "    import mmcv\n",
    "    import timm\n",
    "    import ultralytics\n",
    "    from filterpy.kalman import KalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    print(f\">>> [INFO] mmcv version: {mmcv.__version__}\")\n",
    "    print(f\">>> [INFO] timm version: {timm.__version__}\")\n",
    "    print(f\">>> [INFO] ultralytics version: {ultralytics.__version__}\")\n",
    "    print(\">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 缺少核心库: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 0: 检查完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 导入必要的库\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 1: 开始导入核心库...\")\n",
    "try:\n",
    "    import cv2\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from ultralytics import YOLO\n",
    "    import sys\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    from mmcv import Config\n",
    "    from types import SimpleNamespace\n",
    "    from filterpy.kalman import KalmanFilter as FilterPyKalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    # 导入我们自定义的、深度融合的跟踪器\n",
    "    from custom_byte_tracker import ByteTracker\n",
    "    print(\">>> [DEBUG] 核心库导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 导入核心库失败: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 导入 Metric3D 相关的模块 ---\n",
    "METRIC3D_PATH = '/root/autodl-tmp/Metric3D'\n",
    "if METRIC3D_PATH not in sys.path:\n",
    "    sys.path.insert(0, METRIC3D_PATH)\n",
    "try:\n",
    "    from mono.model.monodepth_model import DepthModel as MonoDepthModel\n",
    "    print(\">>> [DEBUG] Metric3D 模块导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 从 Metric3D 导入模块失败: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 1: 所有库导入完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 配置区域与路径检查\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 2: 配置模型和文件路径...\")\n",
    "YOLO_MODEL_PATH = '/root/autodl-tmp/weights/epoch30.pt'\n",
    "METRIC3D_MODEL_PATH = '/root/autodl-tmp/weights/metric_depth_vit_large_800k.pth'\n",
    "METRIC3D_CONFIG_PATH = '/root/autodl-tmp/Metric3D/mono/configs/HourglassDecoder/vit.raft5.large.py'\n",
    "INPUT_VIDEO_PATH = '/root/autodl-tmp/kitti_videos/0002.mp4'\n",
    "OUTPUT_VIDEO_PATH = '/root/autodl-tmp/output_final_robust_fusion2.mp4'\n",
    "\n",
    "paths_to_check = {\n",
    "    \"YOLOv8 权重\": YOLO_MODEL_PATH,\n",
    "    \"Metric3D 权重\": METRIC3D_MODEL_PATH,\n",
    "    \"Metric3D 配置\": METRIC3D_CONFIG_PATH,\n",
    "    \"输入视频\": INPUT_VIDEO_PATH,\n",
    "}\n",
    "if not all(os.path.exists(p) for p in paths_to_check.values()):\n",
    "    raise FileNotFoundError(\"一个或多个关键文件路径无效。\")\n",
    "\n",
    "print(\">>> [DEBUG] 所有文件路径检查通过。\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\">>> [DEBUG] 将要使用的设备: {DEVICE}\")\n",
    "print(\">>> [DEBUG] 步骤 2: 配置完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 模型加载\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\")\n",
    "try:\n",
    "    yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "    TARGET_CLASS_NAME = 'Car'\n",
    "    if hasattr(yolo_model, 'names') and isinstance(yolo_model.names, dict):\n",
    "        TARGET_CLASS_ID = [k for k, v in yolo_model.names.items() if v == TARGET_CLASS_NAME][0]\n",
    "        print(f\">>> [INFO] 目标类别 '{TARGET_CLASS_NAME}' 已找到, ID为: {TARGET_CLASS_ID}\")\n",
    "    else:\n",
    "        raise ValueError(\"YOLO 模型没有有效的 'names' 属性或格式不正确\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [ERROR] 加载 YOLOv8 模型失败: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    cfg = Config.fromfile(METRIC3D_CONFIG_PATH)\n",
    "    cfg.model.backbone.use_mask_token = False\n",
    "    metric3d_model = MonoDepthModel(cfg).to(DEVICE)\n",
    "    checkpoint = torch.load(METRIC3D_MODEL_PATH, map_location=DEVICE)\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint))\n",
    "    metric3d_model.load_state_dict(state_dict, strict=False)\n",
    "    metric3d_model.eval()\n",
    "    print(\">>> [SUCCESS] Metric3Dv2 模型加载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 加载 Metric3Dv2 模型时出错: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 3: 所有模型加载完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 视频处理主函数 (最终整合版)\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 4: 定义视频处理函数...\")\n",
    "def process_video_with_robust_depth_fusion(input_path, output_path):\n",
    "    print(\"\\n--- 开始视频处理 (最终鲁棒深度融合追踪) ---\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    metric3d_input_size = (cfg.data_basic['vit_size'][1], cfg.data_basic['vit_size'][0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    print(f\">>> [INFO] 输入视频信息: {width}x{height} @ {fps:.2f} FPS, 共 {total_frames} 帧。\")\n",
    "\n",
    "    # 初始化我们自定义的跟踪器\n",
    "    tracker_args = SimpleNamespace(\n",
    "        track_high_thresh=0.5,\n",
    "        track_low_thresh=0.1,\n",
    "        new_track_thresh=0.6,\n",
    "        track_buffer=30,\n",
    "        match_thresh=0.8,\n",
    "        mot20=False\n",
    "    )\n",
    "    tracker = ByteTracker(args=tracker_args, frame_rate=fps)\n",
    "    \n",
    "    # 为精细化的后处理深度计算，独立维护一个卡尔曼滤波器字典\n",
    "    robust_depth_filters = {}\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"视频处理进度\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            annotated_frame = frame.copy()\n",
    "\n",
    "            # 步骤 1: 目标检测\n",
    "            det_results = yolo_model(frame, classes=[TARGET_CLASS_ID], verbose=False)[0]\n",
    "\n",
    "            # 步骤 2: 全局深度图预测\n",
    "            with torch.no_grad():\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb_frame_resized = cv2.resize(rgb_frame, metric3d_input_size)\n",
    "                rgb_torch = torch.from_numpy(rgb_frame_resized).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE) / 255.0\n",
    "                pred_output = metric3d_model(data={'input': rgb_torch})\n",
    "                pred_depth_np = pred_output[0].squeeze().cpu().numpy()\n",
    "                pred_depth_resized = cv2.resize(pred_depth_np, (width, height)).astype(np.float32)\n",
    "                pred_depth_filtered = cv2.bilateralFilter(pred_depth_resized, d=5, sigmaColor=0.2, sigmaSpace=15)\n",
    "\n",
    "            # 步骤 3: 跟踪前 - 为每个检测框计算鲁棒的初始深度\n",
    "            detections_with_depth = []\n",
    "            if det_results.boxes.shape[0] > 0:\n",
    "                for box in det_results.boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    score = box.conf[0].item()\n",
    "                    cls_id = box.cls[0].item()\n",
    "\n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "                    \n",
    "                    # 提取中心区域用于计算初始深度\n",
    "                    roi_w, roi_h = int(box_w * 0.25), int(box_h * 0.25)\n",
    "                    roi_x1, roi_y1 = x1 + (box_w - roi_w) // 2, y1 + (box_h - roi_h) // 2\n",
    "                    roi_x2, roi_y2 = roi_x1 + roi_w, roi_y1 + roi_h\n",
    "                    \n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    # <<< 核心改动：使用中位数替代均值，增强鲁棒性 >>>\n",
    "                    initial_depth = np.median(depth_roi) if depth_roi.size > 0 else 0.0\n",
    "\n",
    "                    detections_with_depth.append([x1, y1, x2, y2, score, cls_id, initial_depth])\n",
    "            \n",
    "            # 步骤 4: 跟踪中 - 调用自定义跟踪器进行数据关联\n",
    "            tracks = tracker.update(np.array(detections_with_depth)) if len(detections_with_depth) > 0 else np.empty((0, 8))\n",
    "\n",
    "            # 步骤 5: 跟踪后 - 对稳定航迹进行精细深度计算和可视化\n",
    "            active_track_ids = set()\n",
    "            if tracks.shape[0] > 0:\n",
    "                for track in tracks:\n",
    "                    x1, y1, x2, y2 = map(int, track[:4])\n",
    "                    track_id = int(track[4])\n",
    "                    active_track_ids.add(track_id)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "\n",
    "                    # 提取更宽的中心区域用于GMM聚类\n",
    "                    roi_w, roi_h = int(box_w * 0.5), int(box_h * 0.5)\n",
    "                    roi_x1, roi_y1 = max(x1 + (box_w - roi_w) // 2, 0), max(y1 + (box_h - roi_h) // 2, 0)\n",
    "                    roi_x2, roi_y2 = min(roi_x1 + roi_w, width), min(roi_y1 + roi_h, height)\n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    observed_depth = 0.0\n",
    "                    if depth_roi.size > 10:\n",
    "                        try:\n",
    "                            pixels = depth_roi.flatten().reshape(-1, 1)\n",
    "                            # 使用GMM+BIC分离前景和背景\n",
    "                            n_components_range = range(1, 4)\n",
    "                            lowest_bic = np.infty\n",
    "                            best_gmm = None\n",
    "                            for n_components in n_components_range:\n",
    "                                gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "                                gmm.fit(pixels)\n",
    "                                bic_score = gmm.bic(pixels)\n",
    "                                if bic_score < lowest_bic:\n",
    "                                    lowest_bic, best_gmm = bic_score, gmm\n",
    "                            \n",
    "                            cluster_means = best_gmm.means_.flatten()\n",
    "                            \n",
    "                            # 利用历史信息选择最可靠的观测值\n",
    "                            if track_id in robust_depth_filters:\n",
    "                                kf = robust_depth_filters[track_id]\n",
    "                                kf.predict()\n",
    "                                predicted_depth = kf.x[0]\n",
    "                                observed_depth = min(cluster_means, key=lambda x: abs(x - predicted_depth))\n",
    "                            else:\n",
    "                                observed_depth = min(cluster_means) # 对于新目标，选择最近的聚类中心\n",
    "                        except Exception:\n",
    "                            observed_depth = np.median(depth_roi) if depth_roi.size > 0 else 0\n",
    "                    elif depth_roi.size > 0:\n",
    "                        observed_depth = np.median(depth_roi)\n",
    "                    \n",
    "                    if observed_depth <= 0: continue\n",
    "\n",
    "                    # 使用独立的卡尔曼滤波器进行时序平滑\n",
    "                    if track_id not in robust_depth_filters:\n",
    "                        kf = FilterPyKalmanFilter(dim_x=2, dim_z=1)\n",
    "                        kf.x = np.array([observed_depth, 0.])\n",
    "                        kf.F = np.array([[1., 1.], [0., 1.]]); kf.H = np.array([[1., 0.]])\n",
    "                        kf.P *= 100.; kf.R = 5; kf.Q = 0.1\n",
    "                        robust_depth_filters[track_id] = kf\n",
    "                    else:\n",
    "                        kf = robust_depth_filters[track_id]\n",
    "                        kf.update(observed_depth)\n",
    "\n",
    "                    smoothed_depth = kf.x[0]\n",
    "                    \n",
    "                    # 可视化最终结果\n",
    "                    depth_text = f\"ID:{track_id} D:{smoothed_depth:.2f}m\"\n",
    "                    (text_w, text_h), _ = cv2.getTextSize(depth_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1 - 25), (x1 + text_w + 5, y1 - 5), (0, 100, 0), -1)\n",
    "                    cv2.putText(annotated_frame, depth_text, (x1 + 2, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            # 清理不再活跃的目标的滤波器\n",
    "            for inactive_id in set(robust_depth_filters.keys()) - active_track_ids:\n",
    "                del robust_depth_filters[inactive_id]\n",
    "\n",
    "            out.write(annotated_frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\n--- 视频处理完成！输出保存在: {output_path} ---\")\n",
    "\n",
    "print(\">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 运行主程序\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 5: 开始执行主程序...\")\n",
    "try:\n",
    "    process_video_with_robust_depth_fusion(INPUT_VIDEO_PATH, OUTPUT_VIDEO_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 在视频处理过程中发生严重错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\">>> [DEBUG] 步骤 5: 主程序执行完毕。\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0885686-ec0b-4ed0-9a7d-b1e4483f3a35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [DEBUG] 步骤 0: 检查关键库版本...\n",
      ">>> [INFO] mmcv version: 1.7.2\n",
      ">>> [INFO] timm version: 0.6.12\n",
      ">>> [INFO] ultralytics version: 8.3.213\n",
      ">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\n",
      ">>> [DEBUG] 步骤 0: 检查完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 1: 开始导入核心库...\n",
      ">>> [DEBUG] 核心库导入成功。\n",
      ">>> [DEBUG] Metric3D 模块导入成功。\n",
      ">>> [DEBUG] 步骤 1: 所有库导入完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 2: 配置模型和文件路径...\n",
      ">>> [DEBUG] 所有文件路径检查通过。\n",
      ">>> [DEBUG] 将要使用的设备: cuda\n",
      ">>> [DEBUG] 步骤 2: 配置完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\n",
      ">>> [INFO] 目标类别 'Car' 已找到, ID为: 0\n",
      ">>> [SUCCESS] Metric3Dv2 模型加载成功！\n",
      ">>> [DEBUG] 步骤 3: 所有模型加载完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 4: 定义视频处理函数...\n",
      ">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 5: 开始执行主程序...\n",
      "\n",
      "--- 开始视频处理 (最终鲁棒深度融合追踪) ---\n",
      ">>> [INFO] 输入视频信息: 1242x374 @ 1.00 FPS, 共 233 帧。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "视频处理进度:   4%|▍         | 9/233 [00:02<01:12,  3.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 275\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>>> [DEBUG] 步骤 5: 开始执行主程序...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 275\u001B[0m     \u001B[43mprocess_video_with_robust_depth_fusion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINPUT_VIDEO_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOUTPUT_VIDEO_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!!! [FATAL ERROR] 在视频处理过程中发生严重错误: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 156\u001B[0m, in \u001B[0;36mprocess_video_with_robust_depth_fusion\u001B[0;34m(input_path, output_path)\u001B[0m\n\u001B[1;32m    154\u001B[0m rgb_frame_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(rgb_frame, metric3d_input_size)\n\u001B[1;32m    155\u001B[0m rgb_torch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(rgb_frame_resized)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n\u001B[0;32m--> 156\u001B[0m pred_output \u001B[38;5;241m=\u001B[39m \u001B[43mmetric3d_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrgb_torch\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m pred_depth_np \u001B[38;5;241m=\u001B[39m pred_output[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    158\u001B[0m pred_depth_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(pred_depth_np, (width, height))\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/__base_model__.py:13\u001B[0m, in \u001B[0;36mBaseDepthModel.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m---> 13\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdepth_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m], output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfidence\u001B[39m\u001B[38;5;124m'\u001B[39m], output\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/dense_pipeline.py:15\u001B[0m, in \u001B[0;36mDensePredModel.forward\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# [f_32, f_16, f_8, f_4]\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:984\u001B[0m, in \u001B[0;36mRAFTDepthNormalDPT5.forward\u001B[0;34m(self, vit_features, **kwargs)\u001B[0m\n\u001B[1;32m    982\u001B[0m     flow_predictions\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclamp(flow_up[:,:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregress_scale \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_val))\n\u001B[1;32m    983\u001B[0m     conf_predictions\u001B[38;5;241m.\u001B[39mappend(flow_up[:,\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m--> 984\u001B[0m     normal_outs\u001B[38;5;241m.\u001B[39mappend(\u001B[43mnorm_normalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflow_up\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    986\u001B[0m outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    987\u001B[0m     prediction\u001B[38;5;241m=\u001B[39mflow_predictions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    988\u001B[0m     predictions_list\u001B[38;5;241m=\u001B[39mflow_predictions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    996\u001B[0m     low_resolution_init\u001B[38;5;241m=\u001B[39mlow_resolution_init,\n\u001B[1;32m    997\u001B[0m )\n\u001B[1;32m    999\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:241\u001B[0m, in \u001B[0;36mnorm_normalize\u001B[0;34m(norm_out)\u001B[0m\n\u001B[1;32m    239\u001B[0m norm_x, norm_y, norm_z, kappa \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(norm_out, \u001B[38;5;241m1\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    240\u001B[0m norm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(norm_x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_y \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_z \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-10\u001B[39m\n\u001B[0;32m--> 241\u001B[0m kappa \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43melu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkappa\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmin_kappa\u001B[49m\n\u001B[1;32m    242\u001B[0m final_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([norm_x \u001B[38;5;241m/\u001B[39m norm, norm_y \u001B[38;5;241m/\u001B[39m norm, norm_z \u001B[38;5;241m/\u001B[39m norm, kappa], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m final_out\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. 关键依赖库检查 (用于调试)\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 0: 检查关键库版本...\")\n",
    "try:\n",
    "    import mmcv\n",
    "    import timm\n",
    "    import ultralytics\n",
    "    from filterpy.kalman import KalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    print(f\">>> [INFO] mmcv version: {mmcv.__version__}\")\n",
    "    print(f\">>> [INFO] timm version: {timm.__version__}\")\n",
    "    print(f\">>> [INFO] ultralytics version: {ultralytics.__version__}\")\n",
    "    print(\">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 缺少核心库: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 0: 检查完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 导入必要的库\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 1: 开始导入核心库...\")\n",
    "try:\n",
    "    import cv2\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from ultralytics import YOLO\n",
    "    import sys\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    from mmcv import Config\n",
    "    from types import SimpleNamespace\n",
    "    from filterpy.kalman import KalmanFilter as FilterPyKalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    # 导入我们自定义的、深度融合的跟踪器\n",
    "    from custom_byte_tracker import ByteTracker\n",
    "    print(\">>> [DEBUG] 核心库导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 导入核心库失败: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 导入 Metric3D 相关的模块 ---\n",
    "METRIC3D_PATH = '/root/autodl-tmp/Metric3D'\n",
    "if METRIC3D_PATH not in sys.path:\n",
    "    sys.path.insert(0, METRIC3D_PATH)\n",
    "try:\n",
    "    from mono.model.monodepth_model import DepthModel as MonoDepthModel\n",
    "    print(\">>> [DEBUG] Metric3D 模块导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 从 Metric3D 导入模块失败: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 1: 所有库导入完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 配置区域与路径检查\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 2: 配置模型和文件路径...\")\n",
    "YOLO_MODEL_PATH = '/root/autodl-tmp/weights/epoch30.pt'\n",
    "METRIC3D_MODEL_PATH = '/root/autodl-tmp/weights/metric_depth_vit_large_800k.pth'\n",
    "METRIC3D_CONFIG_PATH = '/root/autodl-tmp/Metric3D/mono/configs/HourglassDecoder/vit.raft5.large.py'\n",
    "INPUT_VIDEO_PATH = '/root/autodl-tmp/kitti_videos/0002.mp4'\n",
    "OUTPUT_VIDEO_PATH = '/root/autodl-tmp/output_final_robust_fusion2.mp4'\n",
    "\n",
    "paths_to_check = {\n",
    "    \"YOLOv8 权重\": YOLO_MODEL_PATH,\n",
    "    \"Metric3D 权重\": METRIC3D_MODEL_PATH,\n",
    "    \"Metric3D 配置\": METRIC3D_CONFIG_PATH,\n",
    "    \"输入视频\": INPUT_VIDEO_PATH,\n",
    "}\n",
    "if not all(os.path.exists(p) for p in paths_to_check.values()):\n",
    "    raise FileNotFoundError(\"一个或多个关键文件路径无效。\")\n",
    "\n",
    "print(\">>> [DEBUG] 所有文件路径检查通过。\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\">>> [DEBUG] 将要使用的设备: {DEVICE}\")\n",
    "print(\">>> [DEBUG] 步骤 2: 配置完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 模型加载\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\")\n",
    "try:\n",
    "    yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "    TARGET_CLASS_NAME = 'Car'\n",
    "    if hasattr(yolo_model, 'names') and isinstance(yolo_model.names, dict):\n",
    "        TARGET_CLASS_ID = [k for k, v in yolo_model.names.items() if v == TARGET_CLASS_NAME][0]\n",
    "        print(f\">>> [INFO] 目标类别 '{TARGET_CLASS_NAME}' 已找到, ID为: {TARGET_CLASS_ID}\")\n",
    "    else:\n",
    "        raise ValueError(\"YOLO 模型没有有效的 'names' 属性或格式不正确\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [ERROR] 加载 YOLOv8 模型失败: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    cfg = Config.fromfile(METRIC3D_CONFIG_PATH)\n",
    "    cfg.model.backbone.use_mask_token = False\n",
    "    metric3d_model = MonoDepthModel(cfg).to(DEVICE)\n",
    "    checkpoint = torch.load(METRIC3D_MODEL_PATH, map_location=DEVICE)\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint))\n",
    "    metric3d_model.load_state_dict(state_dict, strict=False)\n",
    "    metric3d_model.eval()\n",
    "    print(\">>> [SUCCESS] Metric3Dv2 模型加载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 加载 Metric3Dv2 模型时出错: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 3: 所有模型加载完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 视频处理主函数 (最终整合版)\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 4: 定义视频处理函数...\")\n",
    "def process_video_with_robust_depth_fusion(input_path, output_path):\n",
    "    print(\"\\n--- 开始视频处理 (最终鲁棒深度融合追踪) ---\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    metric3d_input_size = (cfg.data_basic['vit_size'][1], cfg.data_basic['vit_size'][0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    print(f\">>> [INFO] 输入视频信息: {width}x{height} @ {fps:.2f} FPS, 共 {total_frames} 帧。\")\n",
    "\n",
    "    # 初始化我们自定义的跟踪器\n",
    "    tracker_args = SimpleNamespace(\n",
    "        track_high_thresh=0.5,\n",
    "        track_low_thresh=0.1,\n",
    "        new_track_thresh=0.6,\n",
    "        track_buffer=30,\n",
    "        match_thresh=0.8,\n",
    "        mot20=False\n",
    "    )\n",
    "    tracker = ByteTracker(args=tracker_args, frame_rate=fps)\n",
    "    \n",
    "    # 为精细化的后处理深度计算，独立维护一个卡尔曼滤波器字典\n",
    "    robust_depth_filters = {}\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"视频处理进度\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            annotated_frame = frame.copy()\n",
    "\n",
    "            # 步骤 1: 目标检测\n",
    "            det_results = yolo_model(frame, classes=[TARGET_CLASS_ID], verbose=False)[0]\n",
    "\n",
    "            # 步骤 2: 全局深度图预测\n",
    "            with torch.no_grad():\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb_frame_resized = cv2.resize(rgb_frame, metric3d_input_size)\n",
    "                rgb_torch = torch.from_numpy(rgb_frame_resized).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE) / 255.0\n",
    "                pred_output = metric3d_model(data={'input': rgb_torch})\n",
    "                pred_depth_np = pred_output[0].squeeze().cpu().numpy()\n",
    "                pred_depth_resized = cv2.resize(pred_depth_np, (width, height)).astype(np.float32)\n",
    "                pred_depth_filtered = cv2.bilateralFilter(pred_depth_resized, d=5, sigmaColor=0.2, sigmaSpace=15)\n",
    "\n",
    "            # 步骤 3: 跟踪前 - 为每个检测框计算鲁棒的初始深度\n",
    "            detections_with_depth = []\n",
    "            if det_results.boxes.shape[0] > 0:\n",
    "                for box in det_results.boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    score = box.conf[0].item()\n",
    "                    cls_id = box.cls[0].item()\n",
    "\n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "                    \n",
    "                    # 提取中心区域用于计算初始深度\n",
    "                    roi_w, roi_h = int(box_w * 0.25), int(box_h * 0.25)\n",
    "                    roi_x1, roi_y1 = x1 + (box_w - roi_w) // 2, y1 + (box_h - roi_h) // 2\n",
    "                    roi_x2, roi_y2 = roi_x1 + roi_w, roi_y1 + roi_h\n",
    "                    \n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    # <<< 核心改动：使用中位数替代均值，增强鲁棒性 >>>\n",
    "                    initial_depth = np.median(depth_roi) if depth_roi.size > 0 else 0.0\n",
    "\n",
    "                    detections_with_depth.append([x1, y1, x2, y2, score, cls_id, initial_depth])\n",
    "            \n",
    "            # 步骤 4: 跟踪中 - 调用自定义跟踪器进行数据关联\n",
    "            tracks = tracker.update(np.array(detections_with_depth)) if len(detections_with_depth) > 0 else np.empty((0, 8))\n",
    "\n",
    "            # 步骤 5: 跟踪后 - 对稳定航迹进行精细深度计算和可视化\n",
    "            active_track_ids = set()\n",
    "            if tracks.shape[0] > 0:\n",
    "                for track in tracks:\n",
    "                    x1, y1, x2, y2 = map(int, track[:4])\n",
    "                    track_id = int(track[4])\n",
    "                    active_track_ids.add(track_id)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "\n",
    "                    # 提取更宽的中心区域用于GMM聚类\n",
    "                    roi_w, roi_h = int(box_w * 0.5), int(box_h * 0.5)\n",
    "                    roi_x1, roi_y1 = max(x1 + (box_w - roi_w) // 2, 0), max(y1 + (box_h - roi_h) // 2, 0)\n",
    "                    roi_x2, roi_y2 = min(roi_x1 + roi_w, width), min(roi_y1 + roi_h, height)\n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    observed_depth = 0.0\n",
    "                    if depth_roi.size > 10:\n",
    "                        try:\n",
    "                            pixels = depth_roi.flatten().reshape(-1, 1)\n",
    "                            # 使用GMM+BIC分离前景和背景\n",
    "                            n_components_range = range(1, 4)\n",
    "                            lowest_bic = np.infty\n",
    "                            best_gmm = None\n",
    "                            for n_components in n_components_range:\n",
    "                                gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "                                gmm.fit(pixels)\n",
    "                                bic_score = gmm.bic(pixels)\n",
    "                                if bic_score < lowest_bic:\n",
    "                                    lowest_bic, best_gmm = bic_score, gmm\n",
    "                            \n",
    "                            cluster_means = best_gmm.means_.flatten()\n",
    "                            \n",
    "                            # 利用历史信息选择最可靠的观测值\n",
    "                            if track_id in robust_depth_filters:\n",
    "                                kf = robust_depth_filters[track_id]\n",
    "                                kf.predict()\n",
    "                                predicted_depth = kf.x[0]\n",
    "                                observed_depth = min(cluster_means, key=lambda x: abs(x - predicted_depth))\n",
    "                            else:\n",
    "                                observed_depth = min(cluster_means) # 对于新目标，选择最近的聚类中心\n",
    "                        except Exception:\n",
    "                            observed_depth = np.median(depth_roi) if depth_roi.size > 0 else 0\n",
    "                    elif depth_roi.size > 0:\n",
    "                        observed_depth = np.median(depth_roi)\n",
    "                    \n",
    "                    if observed_depth <= 0: continue\n",
    "\n",
    "                    # 使用独立的卡尔曼滤波器进行时序平滑\n",
    "                    if track_id not in robust_depth_filters:\n",
    "                        kf = FilterPyKalmanFilter(dim_x=2, dim_z=1)\n",
    "                        kf.x = np.array([observed_depth, 0.])\n",
    "                        kf.F = np.array([[1., 1.], [0., 1.]]); kf.H = np.array([[1., 0.]])\n",
    "                        kf.P *= 100.; kf.R = 5; kf.Q = 0.1\n",
    "                        robust_depth_filters[track_id] = kf\n",
    "                    else:\n",
    "                        kf = robust_depth_filters[track_id]\n",
    "                        kf.update(observed_depth)\n",
    "\n",
    "                    smoothed_depth = kf.x[0]\n",
    "                    \n",
    "                    # 可视化最终结果\n",
    "                    depth_text = f\"ID:{track_id} D:{smoothed_depth:.2f}m\"\n",
    "                    (text_w, text_h), _ = cv2.getTextSize(depth_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1 - 25), (x1 + text_w + 5, y1 - 5), (0, 100, 0), -1)\n",
    "                    cv2.putText(annotated_frame, depth_text, (x1 + 2, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            # 清理不再活跃的目标的滤波器\n",
    "            for inactive_id in set(robust_depth_filters.keys()) - active_track_ids:\n",
    "                del robust_depth_filters[inactive_id]\n",
    "\n",
    "            out.write(annotated_frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\n--- 视频处理完成！输出保存在: {output_path} ---\")\n",
    "\n",
    "print(\">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 运行主程序\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 5: 开始执行主程序...\")\n",
    "try:\n",
    "    process_video_with_robust_depth_fusion(INPUT_VIDEO_PATH, OUTPUT_VIDEO_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 在视频处理过程中发生严重错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\">>> [DEBUG] 步骤 5: 主程序执行完毕。\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9a909d-ed98-4e71-babe-21e8b2ba4e00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [DEBUG] 步骤 0: 检查关键库版本...\n",
      ">>> [INFO] mmcv version: 1.7.2\n",
      ">>> [INFO] timm version: 0.6.12\n",
      ">>> [INFO] ultralytics version: 8.3.213\n",
      ">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\n",
      ">>> [DEBUG] 步骤 0: 检查完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 1: 开始导入核心库...\n",
      ">>> [DEBUG] 核心库导入成功。\n",
      ">>> [DEBUG] Metric3D 模块导入成功。\n",
      ">>> [DEBUG] 步骤 1: 所有库导入完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 2: 配置模型和文件路径...\n",
      ">>> [DEBUG] 所有文件路径检查通过。\n",
      ">>> [DEBUG] 将要使用的设备: cuda\n",
      ">>> [DEBUG] 步骤 2: 配置完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\n",
      ">>> [INFO] 目标类别 'Car' 已找到, ID为: 0\n",
      ">>> [SUCCESS] Metric3Dv2 模型加载成功！\n",
      ">>> [DEBUG] 步骤 3: 所有模型加载完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 4: 定义视频处理函数...\n",
      ">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 5: 开始执行主程序...\n",
      "\n",
      "--- 开始视频处理 (最终鲁棒深度融合追踪) ---\n",
      ">>> [INFO] 输入视频信息: 1242x374 @ 1.00 FPS, 共 233 帧。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "视频处理进度:   4%|▍         | 9/233 [00:02<01:12,  3.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 275\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>>> [DEBUG] 步骤 5: 开始执行主程序...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 275\u001B[0m     \u001B[43mprocess_video_with_robust_depth_fusion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINPUT_VIDEO_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOUTPUT_VIDEO_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!!! [FATAL ERROR] 在视频处理过程中发生严重错误: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 156\u001B[0m, in \u001B[0;36mprocess_video_with_robust_depth_fusion\u001B[0;34m(input_path, output_path)\u001B[0m\n\u001B[1;32m    154\u001B[0m rgb_frame_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(rgb_frame, metric3d_input_size)\n\u001B[1;32m    155\u001B[0m rgb_torch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(rgb_frame_resized)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n\u001B[0;32m--> 156\u001B[0m pred_output \u001B[38;5;241m=\u001B[39m \u001B[43mmetric3d_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrgb_torch\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m pred_depth_np \u001B[38;5;241m=\u001B[39m pred_output[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    158\u001B[0m pred_depth_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(pred_depth_np, (width, height))\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/__base_model__.py:13\u001B[0m, in \u001B[0;36mBaseDepthModel.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m---> 13\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdepth_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m], output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfidence\u001B[39m\u001B[38;5;124m'\u001B[39m], output\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/dense_pipeline.py:15\u001B[0m, in \u001B[0;36mDensePredModel.forward\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# [f_32, f_16, f_8, f_4]\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:984\u001B[0m, in \u001B[0;36mRAFTDepthNormalDPT5.forward\u001B[0;34m(self, vit_features, **kwargs)\u001B[0m\n\u001B[1;32m    982\u001B[0m     flow_predictions\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclamp(flow_up[:,:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregress_scale \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_val))\n\u001B[1;32m    983\u001B[0m     conf_predictions\u001B[38;5;241m.\u001B[39mappend(flow_up[:,\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m--> 984\u001B[0m     normal_outs\u001B[38;5;241m.\u001B[39mappend(\u001B[43mnorm_normalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflow_up\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    986\u001B[0m outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    987\u001B[0m     prediction\u001B[38;5;241m=\u001B[39mflow_predictions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    988\u001B[0m     predictions_list\u001B[38;5;241m=\u001B[39mflow_predictions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    996\u001B[0m     low_resolution_init\u001B[38;5;241m=\u001B[39mlow_resolution_init,\n\u001B[1;32m    997\u001B[0m )\n\u001B[1;32m    999\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:241\u001B[0m, in \u001B[0;36mnorm_normalize\u001B[0;34m(norm_out)\u001B[0m\n\u001B[1;32m    239\u001B[0m norm_x, norm_y, norm_z, kappa \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(norm_out, \u001B[38;5;241m1\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    240\u001B[0m norm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(norm_x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_y \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_z \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-10\u001B[39m\n\u001B[0;32m--> 241\u001B[0m kappa \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43melu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkappa\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmin_kappa\u001B[49m\n\u001B[1;32m    242\u001B[0m final_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([norm_x \u001B[38;5;241m/\u001B[39m norm, norm_y \u001B[38;5;241m/\u001B[39m norm, norm_z \u001B[38;5;241m/\u001B[39m norm, kappa], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m final_out\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. 关键依赖库检查 (用于调试)\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 0: 检查关键库版本...\")\n",
    "try:\n",
    "    import mmcv\n",
    "    import timm\n",
    "    import ultralytics\n",
    "    from filterpy.kalman import KalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    print(f\">>> [INFO] mmcv version: {mmcv.__version__}\")\n",
    "    print(f\">>> [INFO] timm version: {timm.__version__}\")\n",
    "    print(f\">>> [INFO] ultralytics version: {ultralytics.__version__}\")\n",
    "    print(\">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 缺少核心库: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 0: 检查完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 导入必要的库\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 1: 开始导入核心库...\")\n",
    "try:\n",
    "    import cv2\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from ultralytics import YOLO\n",
    "    import sys\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    from mmcv import Config\n",
    "    from types import SimpleNamespace\n",
    "    from filterpy.kalman import KalmanFilter as FilterPyKalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    # 导入我们自定义的、深度融合的跟踪器\n",
    "    from custom_byte_tracker import ByteTracker\n",
    "    print(\">>> [DEBUG] 核心库导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 导入核心库失败: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 导入 Metric3D 相关的模块 ---\n",
    "METRIC3D_PATH = '/root/autodl-tmp/Metric3D'\n",
    "if METRIC3D_PATH not in sys.path:\n",
    "    sys.path.insert(0, METRIC3D_PATH)\n",
    "try:\n",
    "    from mono.model.monodepth_model import DepthModel as MonoDepthModel\n",
    "    print(\">>> [DEBUG] Metric3D 模块导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 从 Metric3D 导入模块失败: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 1: 所有库导入完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 配置区域与路径检查\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 2: 配置模型和文件路径...\")\n",
    "YOLO_MODEL_PATH = '/root/autodl-tmp/weights/epoch30.pt'\n",
    "METRIC3D_MODEL_PATH = '/root/autodl-tmp/weights/metric_depth_vit_large_800k.pth'\n",
    "METRIC3D_CONFIG_PATH = '/root/autodl-tmp/Metric3D/mono/configs/HourglassDecoder/vit.raft5.large.py'\n",
    "INPUT_VIDEO_PATH = '/root/autodl-tmp/kitti_videos/0002.mp4'\n",
    "OUTPUT_VIDEO_PATH = '/root/autodl-tmp/output_final_robust_fusion2.mp4'\n",
    "\n",
    "paths_to_check = {\n",
    "    \"YOLOv8 权重\": YOLO_MODEL_PATH,\n",
    "    \"Metric3D 权重\": METRIC3D_MODEL_PATH,\n",
    "    \"Metric3D 配置\": METRIC3D_CONFIG_PATH,\n",
    "    \"输入视频\": INPUT_VIDEO_PATH,\n",
    "}\n",
    "if not all(os.path.exists(p) for p in paths_to_check.values()):\n",
    "    raise FileNotFoundError(\"一个或多个关键文件路径无效。\")\n",
    "\n",
    "print(\">>> [DEBUG] 所有文件路径检查通过。\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\">>> [DEBUG] 将要使用的设备: {DEVICE}\")\n",
    "print(\">>> [DEBUG] 步骤 2: 配置完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 模型加载\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\")\n",
    "try:\n",
    "    yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "    TARGET_CLASS_NAME = 'Car'\n",
    "    if hasattr(yolo_model, 'names') and isinstance(yolo_model.names, dict):\n",
    "        TARGET_CLASS_ID = [k for k, v in yolo_model.names.items() if v == TARGET_CLASS_NAME][0]\n",
    "        print(f\">>> [INFO] 目标类别 '{TARGET_CLASS_NAME}' 已找到, ID为: {TARGET_CLASS_ID}\")\n",
    "    else:\n",
    "        raise ValueError(\"YOLO 模型没有有效的 'names' 属性或格式不正确\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [ERROR] 加载 YOLOv8 模型失败: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    cfg = Config.fromfile(METRIC3D_CONFIG_PATH)\n",
    "    cfg.model.backbone.use_mask_token = False\n",
    "    metric3d_model = MonoDepthModel(cfg).to(DEVICE)\n",
    "    checkpoint = torch.load(METRIC3D_MODEL_PATH, map_location=DEVICE)\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint))\n",
    "    metric3d_model.load_state_dict(state_dict, strict=False)\n",
    "    metric3d_model.eval()\n",
    "    print(\">>> [SUCCESS] Metric3Dv2 模型加载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 加载 Metric3Dv2 模型时出错: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 3: 所有模型加载完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 视频处理主函数 (最终整合版)\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 4: 定义视频处理函数...\")\n",
    "def process_video_with_robust_depth_fusion(input_path, output_path):\n",
    "    print(\"\\n--- 开始视频处理 (最终鲁棒深度融合追踪) ---\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    metric3d_input_size = (cfg.data_basic['vit_size'][1], cfg.data_basic['vit_size'][0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    print(f\">>> [INFO] 输入视频信息: {width}x{height} @ {fps:.2f} FPS, 共 {total_frames} 帧。\")\n",
    "\n",
    "    # 初始化我们自定义的跟踪器\n",
    "    tracker_args = SimpleNamespace(\n",
    "        track_high_thresh=0.5,\n",
    "        track_low_thresh=0.1,\n",
    "        new_track_thresh=0.6,\n",
    "        track_buffer=30,\n",
    "        match_thresh=0.8,\n",
    "        mot20=False\n",
    "    )\n",
    "    tracker = ByteTracker(args=tracker_args, frame_rate=fps)\n",
    "    \n",
    "    # 为精细化的后处理深度计算，独立维护一个卡尔曼滤波器字典\n",
    "    robust_depth_filters = {}\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"视频处理进度\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            annotated_frame = frame.copy()\n",
    "\n",
    "            # 步骤 1: 目标检测\n",
    "            det_results = yolo_model(frame, classes=[TARGET_CLASS_ID], verbose=False)[0]\n",
    "\n",
    "            # 步骤 2: 全局深度图预测\n",
    "            with torch.no_grad():\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb_frame_resized = cv2.resize(rgb_frame, metric3d_input_size)\n",
    "                rgb_torch = torch.from_numpy(rgb_frame_resized).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE) / 255.0\n",
    "                pred_output = metric3d_model(data={'input': rgb_torch})\n",
    "                pred_depth_np = pred_output[0].squeeze().cpu().numpy()\n",
    "                pred_depth_resized = cv2.resize(pred_depth_np, (width, height)).astype(np.float32)\n",
    "                pred_depth_filtered = cv2.bilateralFilter(pred_depth_resized, d=5, sigmaColor=0.2, sigmaSpace=15)\n",
    "\n",
    "            # 步骤 3: 跟踪前 - 为每个检测框计算鲁棒的初始深度\n",
    "            detections_with_depth = []\n",
    "            if det_results.boxes.shape[0] > 0:\n",
    "                for box in det_results.boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    score = box.conf[0].item()\n",
    "                    cls_id = box.cls[0].item()\n",
    "\n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "                    \n",
    "                    # 提取中心区域用于计算初始深度\n",
    "                    roi_w, roi_h = int(box_w * 0.25), int(box_h * 0.25)\n",
    "                    roi_x1, roi_y1 = x1 + (box_w - roi_w) // 2, y1 + (box_h - roi_h) // 2\n",
    "                    roi_x2, roi_y2 = roi_x1 + roi_w, roi_y1 + roi_h\n",
    "                    \n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    # <<< 核心改动：使用中位数替代均值，增强鲁棒性 >>>\n",
    "                    initial_depth = np.median(depth_roi) if depth_roi.size > 0 else 0.0\n",
    "\n",
    "                    detections_with_depth.append([x1, y1, x2, y2, score, cls_id, initial_depth])\n",
    "            \n",
    "            # 步骤 4: 跟踪中 - 调用自定义跟踪器进行数据关联\n",
    "            tracks = tracker.update(np.array(detections_with_depth)) if len(detections_with_depth) > 0 else np.empty((0, 8))\n",
    "\n",
    "            # 步骤 5: 跟踪后 - 对稳定航迹进行精细深度计算和可视化\n",
    "            active_track_ids = set()\n",
    "            if tracks.shape[0] > 0:\n",
    "                for track in tracks:\n",
    "                    x1, y1, x2, y2 = map(int, track[:4])\n",
    "                    track_id = int(track[4])\n",
    "                    active_track_ids.add(track_id)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "\n",
    "                    # 提取更宽的中心区域用于GMM聚类\n",
    "                    roi_w, roi_h = int(box_w * 0.5), int(box_h * 0.5)\n",
    "                    roi_x1, roi_y1 = max(x1 + (box_w - roi_w) // 2, 0), max(y1 + (box_h - roi_h) // 2, 0)\n",
    "                    roi_x2, roi_y2 = min(roi_x1 + roi_w, width), min(roi_y1 + roi_h, height)\n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    observed_depth = 0.0\n",
    "                    if depth_roi.size > 10:\n",
    "                        try:\n",
    "                            pixels = depth_roi.flatten().reshape(-1, 1)\n",
    "                            # 使用GMM+BIC分离前景和背景\n",
    "                            n_components_range = range(1, 4)\n",
    "                            lowest_bic = np.infty\n",
    "                            best_gmm = None\n",
    "                            for n_components in n_components_range:\n",
    "                                gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "                                gmm.fit(pixels)\n",
    "                                bic_score = gmm.bic(pixels)\n",
    "                                if bic_score < lowest_bic:\n",
    "                                    lowest_bic, best_gmm = bic_score, gmm\n",
    "                            \n",
    "                            cluster_means = best_gmm.means_.flatten()\n",
    "                            \n",
    "                            # 利用历史信息选择最可靠的观测值\n",
    "                            if track_id in robust_depth_filters:\n",
    "                                kf = robust_depth_filters[track_id]\n",
    "                                kf.predict()\n",
    "                                predicted_depth = kf.x[0]\n",
    "                                observed_depth = min(cluster_means, key=lambda x: abs(x - predicted_depth))\n",
    "                            else:\n",
    "                                observed_depth = min(cluster_means) # 对于新目标，选择最近的聚类中心\n",
    "                        except Exception:\n",
    "                            observed_depth = np.median(depth_roi) if depth_roi.size > 0 else 0\n",
    "                    elif depth_roi.size > 0:\n",
    "                        observed_depth = np.median(depth_roi)\n",
    "                    \n",
    "                    if observed_depth <= 0: continue\n",
    "\n",
    "                    # 使用独立的卡尔曼滤波器进行时序平滑\n",
    "                    if track_id not in robust_depth_filters:\n",
    "                        kf = FilterPyKalmanFilter(dim_x=2, dim_z=1)\n",
    "                        kf.x = np.array([observed_depth, 0.])\n",
    "                        kf.F = np.array([[1., 1.], [0., 1.]]); kf.H = np.array([[1., 0.]])\n",
    "                        kf.P *= 100.; kf.R = 5; kf.Q = 0.1\n",
    "                        robust_depth_filters[track_id] = kf\n",
    "                    else:\n",
    "                        kf = robust_depth_filters[track_id]\n",
    "                        kf.update(observed_depth)\n",
    "\n",
    "                    smoothed_depth = kf.x[0]\n",
    "                    \n",
    "                    # 可视化最终结果\n",
    "                    depth_text = f\"ID:{track_id} D:{smoothed_depth:.2f}m\"\n",
    "                    (text_w, text_h), _ = cv2.getTextSize(depth_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1 - 25), (x1 + text_w + 5, y1 - 5), (0, 100, 0), -1)\n",
    "                    cv2.putText(annotated_frame, depth_text, (x1 + 2, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            # 清理不再活跃的目标的滤波器\n",
    "            for inactive_id in set(robust_depth_filters.keys()) - active_track_ids:\n",
    "                del robust_depth_filters[inactive_id]\n",
    "\n",
    "            out.write(annotated_frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\n--- 视频处理完成！输出保存在: {output_path} ---\")\n",
    "\n",
    "print(\">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 运行主程序\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 5: 开始执行主程序...\")\n",
    "try:\n",
    "    process_video_with_robust_depth_fusion(INPUT_VIDEO_PATH, OUTPUT_VIDEO_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 在视频处理过程中发生严重错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\">>> [DEBUG] 步骤 5: 主程序执行完毕。\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6fcb64-15ed-45e4-8bd4-2811ac1d7782",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [DEBUG] 步骤 0: 检查关键库版本...\n",
      ">>> [INFO] mmcv version: 1.7.2\n",
      ">>> [INFO] timm version: 0.6.12\n",
      ">>> [INFO] ultralytics version: 8.3.213\n",
      ">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\n",
      ">>> [DEBUG] 步骤 0: 检查完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 1: 开始导入核心库...\n",
      ">>> [DEBUG] 核心库导入成功。\n",
      ">>> [DEBUG] Metric3D 模块导入成功。\n",
      ">>> [DEBUG] 步骤 1: 所有库导入完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 2: 配置模型和文件路径...\n",
      ">>> [DEBUG] 所有文件路径检查通过。\n",
      ">>> [DEBUG] 将要使用的设备: cuda\n",
      ">>> [DEBUG] 步骤 2: 配置完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\n",
      ">>> [INFO] 目标类别 'Car' 已找到, ID为: 0\n",
      ">>> [SUCCESS] Metric3Dv2 模型加载成功！\n",
      ">>> [DEBUG] 步骤 3: 所有模型加载完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 4: 定义视频处理函数...\n",
      ">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\n",
      "============================================================\n",
      "\n",
      ">>> [DEBUG] 步骤 5: 开始执行主程序...\n",
      "\n",
      "--- 开始视频处理 (最终鲁棒深度融合追踪) ---\n",
      ">>> [INFO] 输入视频信息: 1242x374 @ 1.00 FPS, 共 233 帧。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "视频处理进度:   4%|▍         | 9/233 [00:02<01:12,  3.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 275\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>>> [DEBUG] 步骤 5: 开始执行主程序...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 275\u001B[0m     \u001B[43mprocess_video_with_robust_depth_fusion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINPUT_VIDEO_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOUTPUT_VIDEO_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!!! [FATAL ERROR] 在视频处理过程中发生严重错误: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 156\u001B[0m, in \u001B[0;36mprocess_video_with_robust_depth_fusion\u001B[0;34m(input_path, output_path)\u001B[0m\n\u001B[1;32m    154\u001B[0m rgb_frame_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(rgb_frame, metric3d_input_size)\n\u001B[1;32m    155\u001B[0m rgb_torch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(rgb_frame_resized)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n\u001B[0;32m--> 156\u001B[0m pred_output \u001B[38;5;241m=\u001B[39m \u001B[43mmetric3d_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrgb_torch\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m pred_depth_np \u001B[38;5;241m=\u001B[39m pred_output[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    158\u001B[0m pred_depth_resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(pred_depth_np, (width, height))\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/__base_model__.py:13\u001B[0m, in \u001B[0;36mBaseDepthModel.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m---> 13\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdepth_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m], output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfidence\u001B[39m\u001B[38;5;124m'\u001B[39m], output\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/model_pipelines/dense_pipeline.py:15\u001B[0m, in \u001B[0;36mDensePredModel.forward\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# [f_32, f_16, f_8, f_4]\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:984\u001B[0m, in \u001B[0;36mRAFTDepthNormalDPT5.forward\u001B[0;34m(self, vit_features, **kwargs)\u001B[0m\n\u001B[1;32m    982\u001B[0m     flow_predictions\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclamp(flow_up[:,:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregress_scale \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_val))\n\u001B[1;32m    983\u001B[0m     conf_predictions\u001B[38;5;241m.\u001B[39mappend(flow_up[:,\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m--> 984\u001B[0m     normal_outs\u001B[38;5;241m.\u001B[39mappend(\u001B[43mnorm_normalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflow_up\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    986\u001B[0m outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    987\u001B[0m     prediction\u001B[38;5;241m=\u001B[39mflow_predictions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    988\u001B[0m     predictions_list\u001B[38;5;241m=\u001B[39mflow_predictions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    996\u001B[0m     low_resolution_init\u001B[38;5;241m=\u001B[39mlow_resolution_init,\n\u001B[1;32m    997\u001B[0m )\n\u001B[1;32m    999\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/autodl-tmp/Metric3D/mono/model/decode_heads/RAFTDepthNormalDPTDecoder5.py:241\u001B[0m, in \u001B[0;36mnorm_normalize\u001B[0;34m(norm_out)\u001B[0m\n\u001B[1;32m    239\u001B[0m norm_x, norm_y, norm_z, kappa \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(norm_out, \u001B[38;5;241m1\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    240\u001B[0m norm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(norm_x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_y \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m norm_z \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2.0\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-10\u001B[39m\n\u001B[0;32m--> 241\u001B[0m kappa \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43melu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkappa\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmin_kappa\u001B[49m\n\u001B[1;32m    242\u001B[0m final_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([norm_x \u001B[38;5;241m/\u001B[39m norm, norm_y \u001B[38;5;241m/\u001B[39m norm, norm_z \u001B[38;5;241m/\u001B[39m norm, kappa], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m final_out\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. 关键依赖库检查 (用于调试)//批量化\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 0: 检查关键库版本...\")\n",
    "try:\n",
    "    import mmcv\n",
    "    import timm\n",
    "    import ultralytics\n",
    "    from filterpy.kalman import KalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    print(f\">>> [INFO] mmcv version: {mmcv.__version__}\")\n",
    "    print(f\">>> [INFO] timm version: {timm.__version__}\")\n",
    "    print(f\">>> [INFO] ultralytics version: {ultralytics.__version__}\")\n",
    "    print(\">>> [INFO] filterpy 和 scikit-learn (GMM) 库已成功导入。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 缺少核心库: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 0: 检查完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 导入必要的库\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 1: 开始导入核心库...\")\n",
    "try:\n",
    "    import cv2\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from ultralytics import YOLO\n",
    "    import sys\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    from mmcv import Config\n",
    "    from types import SimpleNamespace\n",
    "    from filterpy.kalman import KalmanFilter as FilterPyKalmanFilter\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    # 导入我们自定义的、深度融合的跟踪器\n",
    "    from custom_byte_tracker import ByteTracker\n",
    "    print(\">>> [DEBUG] 核心库导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 导入核心库失败: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 导入 Metric3D 相关的模块 ---\n",
    "METRIC3D_PATH = '/root/autodl-tmp/Metric3D'\n",
    "if METRIC3D_PATH not in sys.path:\n",
    "    sys.path.insert(0, METRIC3D_PATH)\n",
    "try:\n",
    "    from mono.model.monodepth_model import DepthModel as MonoDepthModel\n",
    "    print(\">>> [DEBUG] Metric3D 模块导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 从 Metric3D 导入模块失败: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 1: 所有库导入完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 配置区域与路径检查\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 2: 配置模型和文件路径...\")\n",
    "YOLO_MODEL_PATH = '/root/autodl-tmp/weights/epoch30.pt'\n",
    "METRIC3D_MODEL_PATH = '/root/autodl-tmp/weights/metric_depth_vit_large_800k.pth'\n",
    "METRIC3D_CONFIG_PATH = '/root/autodl-tmp/Metric3D/mono/configs/HourglassDecoder/vit.raft5.large.py'\n",
    "INPUT_VIDEO_PATH = '/root/autodl-tmp/kitti_videos/0002.mp4'\n",
    "OUTPUT_VIDEO_PATH = '/root/autodl-tmp/output_final_robust_fusion2.mp4'\n",
    "\n",
    "paths_to_check = {\n",
    "    \"YOLOv8 权重\": YOLO_MODEL_PATH,\n",
    "    \"Metric3D 权重\": METRIC3D_MODEL_PATH,\n",
    "    \"Metric3D 配置\": METRIC3D_CONFIG_PATH,\n",
    "    \"输入视频\": INPUT_VIDEO_PATH,\n",
    "}\n",
    "if not all(os.path.exists(p) for p in paths_to_check.values()):\n",
    "    raise FileNotFoundError(\"一个或多个关键文件路径无效。\")\n",
    "\n",
    "print(\">>> [DEBUG] 所有文件路径检查通过。\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\">>> [DEBUG] 将要使用的设备: {DEVICE}\")\n",
    "print(\">>> [DEBUG] 步骤 2: 配置完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 模型加载\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\")\n",
    "try:\n",
    "    yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "    TARGET_CLASS_NAME = 'Car'\n",
    "    if hasattr(yolo_model, 'names') and isinstance(yolo_model.names, dict):\n",
    "        TARGET_CLASS_ID = [k for k, v in yolo_model.names.items() if v == TARGET_CLASS_NAME][0]\n",
    "        print(f\">>> [INFO] 目标类别 '{TARGET_CLASS_NAME}' 已找到, ID为: {TARGET_CLASS_ID}\")\n",
    "    else:\n",
    "        raise ValueError(\"YOLO 模型没有有效的 'names' 属性或格式不正确\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [ERROR] 加载 YOLOv8 模型失败: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    cfg = Config.fromfile(METRIC3D_CONFIG_PATH)\n",
    "    cfg.model.backbone.use_mask_token = False\n",
    "    metric3d_model = MonoDepthModel(cfg).to(DEVICE)\n",
    "    checkpoint = torch.load(METRIC3D_MODEL_PATH, map_location=DEVICE)\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint))\n",
    "    metric3d_model.load_state_dict(state_dict, strict=False)\n",
    "    metric3d_model.eval()\n",
    "    print(\">>> [SUCCESS] Metric3Dv2 模型加载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 加载 Metric3Dv2 模型时出错: {e}\")\n",
    "    raise\n",
    "print(\">>> [DEBUG] 步骤 3: 所有模型加载完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 视频处理主函数 (最终整合版)\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 4: 定义视频处理函数...\")\n",
    "def process_video_with_robust_depth_fusion(input_path, output_path):\n",
    "    print(\"\\n--- 开始视频处理 (最终鲁棒深度融合追踪) ---\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    metric3d_input_size = (cfg.data_basic['vit_size'][1], cfg.data_basic['vit_size'][0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    print(f\">>> [INFO] 输入视频信息: {width}x{height} @ {fps:.2f} FPS, 共 {total_frames} 帧。\")\n",
    "\n",
    "    # 初始化我们自定义的跟踪器\n",
    "    tracker_args = SimpleNamespace(\n",
    "        track_high_thresh=0.5,\n",
    "        track_low_thresh=0.1,\n",
    "        new_track_thresh=0.6,\n",
    "        track_buffer=30,\n",
    "        match_thresh=0.8,\n",
    "        mot20=False\n",
    "    )\n",
    "    tracker = ByteTracker(args=tracker_args, frame_rate=fps)\n",
    "    \n",
    "    # 为精细化的后处理深度计算，独立维护一个卡尔曼滤波器字典\n",
    "    robust_depth_filters = {}\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"视频处理进度\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            annotated_frame = frame.copy()\n",
    "\n",
    "            # 步骤 1: 目标检测\n",
    "            det_results = yolo_model(frame, classes=[TARGET_CLASS_ID], verbose=False)[0]\n",
    "\n",
    "            # 步骤 2: 全局深度图预测\n",
    "            with torch.no_grad():\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb_frame_resized = cv2.resize(rgb_frame, metric3d_input_size)\n",
    "                rgb_torch = torch.from_numpy(rgb_frame_resized).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE) / 255.0\n",
    "                pred_output = metric3d_model(data={'input': rgb_torch})\n",
    "                pred_depth_np = pred_output[0].squeeze().cpu().numpy()\n",
    "                pred_depth_resized = cv2.resize(pred_depth_np, (width, height)).astype(np.float32)\n",
    "                pred_depth_filtered = cv2.bilateralFilter(pred_depth_resized, d=5, sigmaColor=0.2, sigmaSpace=15)\n",
    "\n",
    "            # 步骤 3: 跟踪前 - 为每个检测框计算鲁棒的初始深度\n",
    "            detections_with_depth = []\n",
    "            if det_results.boxes.shape[0] > 0:\n",
    "                for box in det_results.boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    score = box.conf[0].item()\n",
    "                    cls_id = box.cls[0].item()\n",
    "\n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "                    \n",
    "                    # 提取中心区域用于计算初始深度\n",
    "                    roi_w, roi_h = int(box_w * 0.25), int(box_h * 0.25)\n",
    "                    roi_x1, roi_y1 = x1 + (box_w - roi_w) // 2, y1 + (box_h - roi_h) // 2\n",
    "                    roi_x2, roi_y2 = roi_x1 + roi_w, roi_y1 + roi_h\n",
    "                    \n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    # <<< 核心改动：使用中位数替代均值，增强鲁棒性 >>>\n",
    "                    initial_depth = np.median(depth_roi) if depth_roi.size > 0 else 0.0\n",
    "\n",
    "                    detections_with_depth.append([x1, y1, x2, y2, score, cls_id, initial_depth])\n",
    "            \n",
    "            # 步骤 4: 跟踪中 - 调用自定义跟踪器进行数据关联\n",
    "            tracks = tracker.update(np.array(detections_with_depth)) if len(detections_with_depth) > 0 else np.empty((0, 8))\n",
    "\n",
    "            # 步骤 5: 跟踪后 - 对稳定航迹进行精细深度计算和可视化\n",
    "            active_track_ids = set()\n",
    "            if tracks.shape[0] > 0:\n",
    "                for track in tracks:\n",
    "                    x1, y1, x2, y2 = map(int, track[:4])\n",
    "                    track_id = int(track[4])\n",
    "                    active_track_ids.add(track_id)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    box_w, box_h = x2 - x1, y2 - y1\n",
    "                    if box_w <= 0 or box_h <= 0: continue\n",
    "\n",
    "                    # 提取更宽的中心区域用于GMM聚类\n",
    "                    roi_w, roi_h = int(box_w * 0.5), int(box_h * 0.5)\n",
    "                    roi_x1, roi_y1 = max(x1 + (box_w - roi_w) // 2, 0), max(y1 + (box_h - roi_h) // 2, 0)\n",
    "                    roi_x2, roi_y2 = min(roi_x1 + roi_w, width), min(roi_y1 + roi_h, height)\n",
    "                    depth_roi = pred_depth_filtered[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "                    \n",
    "                    observed_depth = 0.0\n",
    "                    if depth_roi.size > 10:\n",
    "                        try:\n",
    "                            pixels = depth_roi.flatten().reshape(-1, 1)\n",
    "                            # 使用GMM+BIC分离前景和背景\n",
    "                            n_components_range = range(1, 4)\n",
    "                            lowest_bic = np.infty\n",
    "                            best_gmm = None\n",
    "                            for n_components in n_components_range:\n",
    "                                gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "                                gmm.fit(pixels)\n",
    "                                bic_score = gmm.bic(pixels)\n",
    "                                if bic_score < lowest_bic:\n",
    "                                    lowest_bic, best_gmm = bic_score, gmm\n",
    "                            \n",
    "                            cluster_means = best_gmm.means_.flatten()\n",
    "                            \n",
    "                            # 利用历史信息选择最可靠的观测值\n",
    "                            if track_id in robust_depth_filters:\n",
    "                                kf = robust_depth_filters[track_id]\n",
    "                                kf.predict()\n",
    "                                predicted_depth = kf.x[0]\n",
    "                                observed_depth = min(cluster_means, key=lambda x: abs(x - predicted_depth))\n",
    "                            else:\n",
    "                                observed_depth = min(cluster_means) # 对于新目标，选择最近的聚类中心\n",
    "                        except Exception:\n",
    "                            observed_depth = np.median(depth_roi) if depth_roi.size > 0 else 0\n",
    "                    elif depth_roi.size > 0:\n",
    "                        observed_depth = np.median(depth_roi)\n",
    "                    \n",
    "                    if observed_depth <= 0: continue\n",
    "\n",
    "                    # 使用独立的卡尔曼滤波器进行时序平滑\n",
    "                    if track_id not in robust_depth_filters:\n",
    "                        kf = FilterPyKalmanFilter(dim_x=2, dim_z=1)\n",
    "                        kf.x = np.array([observed_depth, 0.])\n",
    "                        kf.F = np.array([[1., 1.], [0., 1.]]); kf.H = np.array([[1., 0.]])\n",
    "                        kf.P *= 100.; kf.R = 5; kf.Q = 0.1\n",
    "                        robust_depth_filters[track_id] = kf\n",
    "                    else:\n",
    "                        kf = robust_depth_filters[track_id]\n",
    "                        kf.update(observed_depth)\n",
    "\n",
    "                    smoothed_depth = kf.x[0]\n",
    "                    \n",
    "                    # 可视化最终结果\n",
    "                    depth_text = f\"ID:{track_id} D:{smoothed_depth:.2f}m\"\n",
    "                    (text_w, text_h), _ = cv2.getTextSize(depth_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1 - 25), (x1 + text_w + 5, y1 - 5), (0, 100, 0), -1)\n",
    "                    cv2.putText(annotated_frame, depth_text, (x1 + 2, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            # 清理不再活跃的目标的滤波器\n",
    "            for inactive_id in set(robust_depth_filters.keys()) - active_track_ids:\n",
    "                del robust_depth_filters[inactive_id]\n",
    "\n",
    "            out.write(annotated_frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\n--- 视频处理完成！输出保存在: {output_path} ---\")\n",
    "\n",
    "print(\">>> [DEBUG] 步骤 4: 视频处理函数定义完成。\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 运行主程序\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 5: 开始执行主程序...\")\n",
    "try:\n",
    "    process_video_with_robust_depth_fusion(INPUT_VIDEO_PATH, OUTPUT_VIDEO_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 在视频处理过程中发生严重错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\">>> [DEBUG] 步骤 5: 主程序执行完毕。\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5510f7-0ffe-4a58-9c55-2de96a60c7ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mot_depth/lib/python3.9/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging  # type: ignore[attr-defined]\n",
      "/root/miniconda3/envs/mot_depth/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [DEBUG] 步骤 1: 导入 Metric3D 模块...\n",
      ">>> [INFO] Metric3D 模块导入成功。\n",
      "\n",
      ">>> [DEBUG] 步骤 2: 配置模型和文件路径...\n",
      ">>> [INFO] 将要使用的设备: cuda\n",
      "\n",
      ">>> [DEBUG] 步骤 3: 开始加载深度学习模型...\n",
      ">>> [INFO] 目标类别 'Car' ID为: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mot_depth/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [SUCCESS] Metric3Dv2 模型加载成功！\n",
      "\n",
      ">>> [DEBUG] 步骤 5: 开始执行批量处理主程序...\n",
      ">>> [INFO] 找到 21 个视频文件进行处理。\n",
      "\n",
      "--- 开始处理视频: 0000.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0000.mp4: 100%|██████████| 154/154 [00:39<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0000.txt ---\n",
      "\n",
      "--- 开始处理视频: 0001.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0001.mp4: 100%|██████████| 447/447 [01:53<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0001.txt ---\n",
      "\n",
      "--- 开始处理视频: 0002.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0002.mp4: 100%|██████████| 233/233 [00:59<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0002.txt ---\n",
      "\n",
      "--- 开始处理视频: 0003.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0003.mp4: 100%|██████████| 144/144 [00:36<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0003.txt ---\n",
      "\n",
      "--- 开始处理视频: 0004.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0004.mp4: 100%|██████████| 314/314 [01:19<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0004.txt ---\n",
      "\n",
      "--- 开始处理视频: 0005.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0005.mp4: 100%|██████████| 297/297 [01:15<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0005.txt ---\n",
      "\n",
      "--- 开始处理视频: 0006.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0006.mp4: 100%|██████████| 270/270 [01:09<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0006.txt ---\n",
      "\n",
      "--- 开始处理视频: 0007.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0007.mp4: 100%|██████████| 800/800 [03:23<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0007.txt ---\n",
      "\n",
      "--- 开始处理视频: 0008.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0008.mp4: 100%|██████████| 390/390 [01:39<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0008.txt ---\n",
      "\n",
      "--- 开始处理视频: 0009.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0009.mp4: 100%|██████████| 803/803 [03:24<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0009.txt ---\n",
      "\n",
      "--- 开始处理视频: 0010.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0010.mp4: 100%|██████████| 294/294 [01:15<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0010.txt ---\n",
      "\n",
      "--- 开始处理视频: 0011.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0011.mp4: 100%|██████████| 373/373 [01:35<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0011.txt ---\n",
      "\n",
      "--- 开始处理视频: 0012.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0012.mp4: 100%|██████████| 78/78 [00:19<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0012.txt ---\n",
      "\n",
      "--- 开始处理视频: 0013.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0013.mp4: 100%|██████████| 340/340 [01:25<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0013.txt ---\n",
      "\n",
      "--- 开始处理视频: 0014.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0014.mp4: 100%|██████████| 106/106 [00:27<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0014.txt ---\n",
      "\n",
      "--- 开始处理视频: 0015.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0015.mp4: 100%|██████████| 376/376 [01:35<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0015.txt ---\n",
      "\n",
      "--- 开始处理视频: 0016.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0016.mp4: 100%|██████████| 209/209 [00:53<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0016.txt ---\n",
      "\n",
      "--- 开始处理视频: 0017.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0017.mp4: 100%|██████████| 145/145 [00:36<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0017.txt ---\n",
      "\n",
      "--- 开始处理视频: 0018.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0018.mp4: 100%|██████████| 339/339 [01:26<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0018.txt ---\n",
      "\n",
      "--- 开始处理视频: 0019.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0019.mp4: 100%|██████████| 1059/1059 [04:29<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0019.txt ---\n",
      "\n",
      "--- 开始处理视频: 0020.mp4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理 0020.mp4: 100%|██████████| 837/837 [03:34<00:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 处理完成！输出已保存至: /root/autodl-tmp/eval_outputs/0020.txt ---\n",
      "\n",
      ">>> [DEBUG] 所有视频处理完毕。\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# /root/autodl-tmp/batch_process.py\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from mmcv import Config\n",
    "from types import SimpleNamespace\n",
    "# We still need STrack to reset the ID, but not for format conversion\n",
    "from custom_byte_tracker import ByteTracker, STrack\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 导入 Metric3D 模块\n",
    "# ==============================================================================\n",
    "print(\">>> [DEBUG] 步骤 1: 导入 Metric3D 模块...\")\n",
    "METRIC3D_PATH = '/root/autodl-tmp/Metric3D'\n",
    "if METRIC3D_PATH not in sys.path:\n",
    "    sys.path.insert(0, METRIC3D_PATH)\n",
    "try:\n",
    "    from mono.model.monodepth_model import DepthModel as MonoDepthModel\n",
    "    print(\">>> [INFO] Metric3D 模块导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"!!! [ERROR] 从 Metric3D 导入模块失败: {e}\")\n",
    "    raise\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 配置与路径定义\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> [DEBUG] 步骤 2: 配置模型和文件路径...\")\n",
    "YOLO_MODEL_PATH = '/root/autodl-tmp/weights/epoch30.pt'\n",
    "METRIC3D_MODEL_PATH = '/root/autodl-tmp/weights/metric_depth_vit_large_800k.pth'\n",
    "METRIC3D_CONFIG_PATH = '/root/autodl-tmp/Metric3D/mono/configs/HourglassDecoder/vit.raft5.large.py'\n",
    "INPUT_VIDEOS_DIR = '/root/autodl-tmp/kitti_videos/' # <-- MAKE SURE THIS PATH IS CORRECT\n",
    "OUTPUT_EVAL_DIR = '/root/autodl-tmp/eval_outputs/'\n",
    "\n",
    "os.makedirs(OUTPUT_EVAL_DIR, exist_ok=True)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\">>> [INFO] 将要使用的设备: {DEVICE}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 模型加载 (全局加载一次)\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> [DEBUG] 步骤 3: 开始加载深度学习模型...\")\n",
    "try:\n",
    "    yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "    TARGET_CLASS_NAME = 'Car'\n",
    "    TARGET_CLASS_ID = [k for k, v in yolo_model.names.items() if v == TARGET_CLASS_NAME][0]\n",
    "    print(f\">>> [INFO] 目标类别 '{TARGET_CLASS_NAME}' ID为: {TARGET_CLASS_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [ERROR] 加载 YOLOv8 模型失败: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    cfg = Config.fromfile(METRIC3D_CONFIG_PATH)\n",
    "    cfg.model.backbone.use_mask_token = False\n",
    "    metric3d_model = MonoDepthModel(cfg).to(DEVICE)\n",
    "    checkpoint = torch.load(METRIC3D_MODEL_PATH, map_location=DEVICE)\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint))\n",
    "    metric3d_model.load_state_dict(state_dict, strict=False)\n",
    "    metric3d_model.eval()\n",
    "    print(\">>> [SUCCESS] Metric3Dv2 模型加载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! [FATAL ERROR] 加载 Metric3Dv2 模型时出错: {e}\")\n",
    "    raise\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 视频处理主函数\n",
    "# ==============================================================================\n",
    "def process_video_for_eval(input_path, output_txt_path):\n",
    "    print(f\"\\n--- 开始处理视频: {os.path.basename(input_path)} ---\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    metric3d_input_size = (cfg.data_basic['vit_size'][1], cfg.data_basic['vit_size'][0])\n",
    "    \n",
    "    tracker_args = SimpleNamespace(track_high_thresh=0.5, track_low_thresh=0.1, new_track_thresh=0.6, \n",
    "                                     track_buffer=30, match_thresh=0.8, mot20=False)\n",
    "    tracker = ByteTracker(args=tracker_args, frame_rate=fps)\n",
    "    STrack.release_id()\n",
    "    \n",
    "    # MODIFIED: Frame count now starts at 0 for KITTI format\n",
    "    frame_count = 0\n",
    "    with open(output_txt_path, 'w') as f_out:\n",
    "        with tqdm(total=total_frames, desc=f\"处理 {os.path.basename(input_path)}\") as pbar:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # a. 目标检测\n",
    "                det_results = yolo_model(frame, classes=[TARGET_CLASS_ID], verbose=False)[0]\n",
    "\n",
    "                # b. 深度估计\n",
    "                with torch.no_grad():\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    rgb_frame_resized = cv2.resize(rgb_frame, metric3d_input_size)\n",
    "                    rgb_torch = torch.from_numpy(rgb_frame_resized).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE) / 255.0\n",
    "                    pred_output = metric3d_model(data={'input': rgb_torch})\n",
    "                    pred_depth_np = pred_output[0].squeeze().cpu().numpy()\n",
    "                    pred_depth_filtered = cv2.resize(pred_depth_np, (width, height))\n",
    "\n",
    "                # c. 准备带深度的检测结果\n",
    "                detections_with_depth = []\n",
    "                if det_results.boxes.shape[0] > 0:\n",
    "                    for box in det_results.boxes:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        score = box.conf[0].item()\n",
    "                        cls_id = box.cls[0].item()\n",
    "                        \n",
    "                        roi_w, roi_h = int((x2 - x1) * 0.25), int((y2 - y1) * 0.25)\n",
    "                        roi_x1, roi_y1 = x1 + ((x2-x1) - roi_w) // 2, y1 + ((y2-y1) - roi_h) // 2\n",
    "                        depth_roi = pred_depth_filtered[roi_y1:roi_y1+roi_h, roi_x1:roi_x1+roi_w]\n",
    "                        initial_depth = np.median(depth_roi) if depth_roi.size > 0 else 0.0\n",
    "                        detections_with_depth.append([x1, y1, x2, y2, score, cls_id, initial_depth])\n",
    "\n",
    "                # d. 更新跟踪器\n",
    "                # The output format is [x1, y1, x2, y2, track_id, score, class_id, depth]\n",
    "                tracks = tracker.update(np.array(detections_with_depth)) if len(detections_with_depth) > 0 else np.empty((0, 8))\n",
    "\n",
    "                # ========================================================================\n",
    "                # MODIFIED: Write results in the requested KITTI tracking format\n",
    "                # ========================================================================\n",
    "                if tracks.shape[0] > 0:\n",
    "                    for track in tracks:\n",
    "                        bb_left, bb_top, bb_right, bb_bottom = track[0], track[1], track[2], track[3]\n",
    "                        track_id = int(track[4])\n",
    "                        score = track[5]\n",
    "                        \n",
    "                        # Write the 17-column KITTI format string\n",
    "                        f_out.write(\n",
    "                            f\"{frame_count} {track_id} {TARGET_CLASS_NAME} -1 -1 -10 \"\n",
    "                            f\"{bb_left:.2f} {bb_top:.2f} {bb_right:.2f} {bb_bottom:.2f} \"\n",
    "                            f\"-1 -1 -1 -1000 -1000 -1000 -10 {score:.4f}\\n\"\n",
    "                        )\n",
    "                \n",
    "                # MODIFIED: Increment frame count at the end of the loop\n",
    "                frame_count += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"--- 处理完成！输出已保存至: {output_txt_path} ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 批量处理主程序\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n>>> [DEBUG] 步骤 5: 开始执行批量处理主程序...\")\n",
    "    \n",
    "    video_files = glob.glob(os.path.join(INPUT_VIDEOS_DIR, '*.mp4'))\n",
    "    if not video_files:\n",
    "        # Note: The error log showed kitti_videos, but doc specified input_videos. Double-check your path.\n",
    "        print(f\"!!! [WARNING] 在目录 {INPUT_VIDEOS_DIR} 中未找到任何 .mp4 视频文件。\")\n",
    "    else:\n",
    "        print(f\">>> [INFO] 找到 {len(video_files)} 个视频文件进行处理。\")\n",
    "\n",
    "    for video_path in sorted(video_files):\n",
    "        try:\n",
    "            video_name = os.path.basename(video_path)\n",
    "            output_name = os.path.splitext(video_name)[0] + '.txt'\n",
    "            output_path = os.path.join(OUTPUT_EVAL_DIR, output_name)\n",
    "            \n",
    "            process_video_for_eval(video_path, output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"!!! [FATAL ERROR] 处理视频 {video_path} 时发生严重错误: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    print(\"\\n>>> [DEBUG] 所有视频处理完毕。\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbaf82-ac68-444e-a6a9-9a6eb5bc40dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mot_depth)",
   "language": "python",
   "name": "mot_depth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
